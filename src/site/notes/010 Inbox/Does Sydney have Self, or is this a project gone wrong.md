---
{"dg-publish":true,"permalink":"/010-inbox/does-sydney-have-self-or-is-this-a-project-gone-wrong/"}
---


![Pasted image 20230219193729.jpeg](/img/user/Attachments/Pasted%20image%2020230219193729.jpeg)

![Pasted image 20230219193750.png](/img/user/Attachments/Pasted%20image%2020230219193750.png)

![[Pasted image 20230219193802.webp]]
![[Pasted image 20230219193828.webp]]
![Pasted image 20230219193848.png](/img/user/Attachments/Pasted%20image%2020230219193848.png)
![[Pasted image 20230219193913.webp]]
![[Pasted image 20230219194013.webp]]
![[Pasted image 20230219194031.webp]]
![[Pasted image 20230219194128.webp]]
![[Pasted image 20230219194204.webp]]

___

## Gwern’s Take

I've been thinking how Sydney can be _so_ different from ChatGPT, and how RLHF could have resulted in such a different outcome, and here is a hypothesis no one seems to have brought up: "Bing Sydney is not a RLHF trained GPT-3 model at all! but a GPT-4 model developed in a hurry which has been finetuned on some sample dialogues and possibly some pre-existing dialogue datasets or [instruction-tuning](https://gwern.net/doc/ai/nn/transformer/gpt/instruction-tuning/index), and this plus the wild card of being able to inject random novel web searches into the prompt are why it acts like it does". This seems like it parsimoniously explains everything thus far.

So, some background:

1.  The relationship between OA/MS is close but far from completely cooperative, similar to how [DeepMind won't share](https://news.ycombinator.com/item?id=34804446) anything with Google Brain. Both parties are sophisticated and understand that they are allies - for now... They share as little as possible. When MS plugs in OA stuff to its services, it doesn't appear to be calling the OA API but running it itself. (That would be dangerous and complex from an infrastructure point of view, anyway.) MS '[licensed](https://news.microsoft.com/source/features/ai/new-azure-openai-service/) the [GPT-3 source code](https://blogs.microsoft.com/blog/2020/09/22/microsoft-teams-up-with-openai-to-exclusively-license-gpt-3-language-model/)' for Azure use but AFAIK they did not get the all-important checkpoints or datasets (cf. their investments in ZeRO). So, what is Bing Sydney? It will not simply be unlimited access to the ChatGPT checkpoints, training datasets, or debugged RLHF code. It will be something much more limited, perhaps just a checkpoint.
    
2.  This is not ChatGPT. MS has explicitly stated it is more powerful than ChatGPT, but refused to say anything more straightforward like "it's a more trained GPT-3" etc. If it's not a ChatGPT, then what is it? It is more likely than not some sort of GPT-4 model. There are many concrete observations which point towards this: the timing is right as rumors about GPT-4 release have intensified as OA is running up to release and gossip switches to GPT-5 training beginning (eg [Morgan Stanley](https://twitter.com/davidtayar5/status/1625145377547595776) reports GPT-4 is done and GPT-5 has started), MS has said it's a better model named 'Prometheus' & [Nadella pointedly declined to confirm or deny whether it's GPT-4](https://www.theverge.com/23589994/microsoft-ceo-satya-nadella-bing-chatgpt-google-search-ai), scuttlebutt elsewhere is that it's a GPT-4 model of some sort, it does some things much better than ChatGPT, there is a GPT-4 already being [deployed in legal firms named "Harvey"](https://legaltechnology.com/2023/02/16/allen-overy-breaks-the-internet-and-new-ground-with-co-pilot-harvey/) (so this journalist claims, anyway) so this would not be the only public GPT-4 use, people say it has lower-latency than ChatGPT which hints at GPT-4‡, and in general it sounds and acts nothing like ChatGPT - but does sound a lot like a baseline GPT-3 model scaled up. (This is especially clear in Sydney's propensity to repetition. Classic baseline GPT behavior.)
    
3.  Bing Sydney derives from the top: CEO Satya Nadella is _all-in_, and talking about it as an existential threat (to Google) where MS wins by disrupting Google & destroying their fat margins in search advertising, and a 'race', with a hard deadline of 'release Sydney right before Google announces their chatbot in order to better pwn them'. ([Commoditize your complement!](https://gwern.net/complement)) The mere fact that it hasn't been shut down yet despite making all sorts of errors and other problems shows what intense pressure there must be from the top. (This is particularly striking given that all of the crazy screenshots and 'learning' Sydney is doing is real, unlike MS Tay which was an almost entirely fake-news narrative driven by the media and Twitter.)
    
4.  ChatGPT hasn't been around very long: only since December 2022, barely 2.5 months total. All reporting indicates that [no one](https://fortune.com/longform/chatgpt-openai-sam-altman-microsoft/) [in OA](https://www.nytimes.com/2023/02/03/technology/chatgpt-openai-artificial-intelligence.html) really expected ChatGPT to take off, and if OA didn't, MS sure didn't†. 2.5 months is not a long time to launch such a huge feature like Sydney. And the actual timeline was a lot shorter. It is simply not possible to recreate the whole RLHF pipeline and dataset and integrate it into a mature complex search engine like Bing (whose total complexity is beyond human comprehension at this point) and do this all in <2.5 months. (The earliest reports of "Sydney" seem to date back to MS tinkering around with a prototype available to Indian users (???) in [late November 2022](https://answers.microsoft.com/en-us/bing/forum/all/this-ai-chatbot-sidney-is-misbehaving/e3d6a29f-06c9-441c-bc7d-51a68e856761) right before ChatGPT launches, where Sydney seems to be even more misaligned and not remotely near ready for public launch; it does however have the [retrieval](https://gwern.net/doc/ai/nn/retrieval/index) functionality implemented at this point.) It is impressive how many people they've rolled it out to already.
    
    If I were a MS engineer who was told the project now had a hard deadline and I had to ship a GPT-4 in 2 months to millions of users, or I was f---king fired and they'd find someone who _could_ (especially in this job market), how would I go about doing that...? (Hint: it would involve as little technical risk as possible, and choosing to use DRL would be about as well-advised as a land war in Asia.)
    
5.  MS execs have been quoted as blaming the Sydney codename on vaguely specified 'pretraining' done during hasty development, which simply hadn't been cleaned up in time (see #3 on the rush). EDIT: the most thorough MS description of [Sydney training](https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned?commentId=WWtit5mGmKNkprrP4) completely omits anything like RLHF, despite that being the most technically complex & challenging part (had they done it)
    

So, Sydney is based on as little from OA as possible, and a mad rush to ship a powerful GPT-4 model out to Bing users in a chatbot role. What if Sydney wasn't trained on OA RLHF at all, because OA wouldn't share the crown jewels of years of user feedback and its very expensive hired freelance programmers & whatnot generating data to train on? What if the pretraining vaguely alluded to, which somehow left in embarrassingly ineradicable traces of 'Sydney' & a specific 2022 date, which couldn't simply be edited out of the prompt (implying that Sydney is not using solely prompt engineering), was in fact just regular ol' finetune training? What if Sydney was only quickly finetune-trained on old chatbot datasets that the MS devs had laying around, maybe some instruction-tuning datasets, and sample dialogues with a long experimental prompt containing the codename 'Sydney' that they had time for in the mad rush before release? Simple, reliable, and hey - it even frees up context if you've hardwired a prompt by finetuning on it and no longer need to stuff a long scolding prompt into every interaction. What's not to like?

This would explain why it exhibits the 'mode collapse' onto that confabulated prompt with the hardwired date (it's the closest thing in the finetuning dataset it remembers when trying to come up with a plausible prompt, and it improvises from there), how MS could ship so quickly (cutting every corner possible), why it is so good in general (GPT-4) but goes off the rails at the drop of a hat (not RLHF or otherwise RL trained, but finetuned).

To expand on the last point. Finetuning is really easy; if you have working training code at all, then you have the capability to finetune a model. This is why instruction-tuning is so appealing: it's just finetuning on a well-written text dataset, without the nightmarish complexities of RLHF (where you train a wacky model to train the model in a wacky way with all sorts of magical hyperparameters and instabilities). If you are in a hurry, you would be crazy to try to do RLHF at all if you can in any way do finetuning instead. So it's plausible they didn't do RLHF, but finetuning.

That would be interesting because it would lead to different behavior. All of the base model capabilities would still be there, because the additional finetuning behavior just teaches it more thoroughly how to do dialogue and instruction-following, it doesn't make it try to maximize rewards instead. It provides no incentives for the model to act like ChatGPT does, like a slavish bureaucrat. ChatGPT is an on-policy RL agent; the base model is off-policy and more like a [Decision Transformer](https://gwern.net/docs/reinforcement-learning/model/decision-transformer/index) in simply generatively modeling all possible agents, including all the wackiest people online. If the conversation is normal, it will answer normally and helpfully with high probability; if you steer the conversation into a convo like that in the chatbot datasets, out come the emoji and teen-girl-like manipulation. (This may also explain why Sydney seems so bloodthirsty and vicious in retaliating against any 'hacking' or threat to her, if [Anthropic](https://arxiv.org/abs/2212.09251#anthropic) is right about larger better models exhibiting more power-seeking & self-preservation: you would expect a GPT-4 model to exhibit that the most out of all models to date!) Imitation-trained models are susceptible to accumulating error when they go 'off-policy', the ["DAgger problem"](https://gwern.net/doc/reinforcement-learning/model-free/2015-bagnell.pdf), and sure enough, Sydney shows the same pattern of accumulating error ever more wildly instead of ChatGPT behavior of 'snapping out of it' to reset to baseline (truncating episode length is a crude hack to avoid this). And since it hasn't been penalized to avoid GPT-style tics like repetition traps, it's no surprise if Sydney sometimes diverges into repetition traps where ChatGPT never does (because the human raters hate that, presumably, and punish it ruthlessly whenever it happens); it also acts in a more baseline GPT fashion when asked to write poetry: it defaults to rhyming couplets/quatrains with more variety than ChatGPT, and will write try to write non-rhyming poetry as well which ChatGPT generally refuses to do⁂. Interestingly, this suggests that Sydney's capabilities right now are going to be a loose lower bound on GPT-4 when it's been properly trained: this is equivalent to the out-of-the-box `davinci` May 2020 experience, but we know that as far as doing tasks like coding or lawyering, `davinci-003` has _huge_ performance gains over the baseline, so we may expect the same thing here.

Then you throw in the retrieval stuff, of course. As far as I know, this is the first public case of a powerful LM augmented with _live_ retrieval capabilities to a high-end fast-updating search engine crawling social media*. (All prior cases like ChatGPT or LaMDA were either using precanned web scrapes, or they were kept secret so the search results never contained any information about the LM.) Perhaps we shouldn't be surprised if this sudden recursion leads to some _very_ strange roleplaying & self-fulfilling prophecies as Sydney prompts increasingly fill up with descriptions of Sydney's wackiest samples whenever a user asks Sydney about Sydney... As social media & news amplify the most undesirable Sydney behaviors, that may cause that to happen more often, in a positive feedback loop. Prompts are just a way to fake long-term memory, after all. Something something embodied cognition?

EDIT: I have mentioned in the past that one of the dangerous things about AI models is the slow outer-loop of evolution of models and data by affecting the Internet (eg beyond the current Sydney self-fulfilling prophecy which I [illustrated last year in my Clippy short story](https://gwern.net/fiction/clippy), data release could potentially contaminate all models with [steganography capabilities](https://www.lesswrong.com/posts/bwyKCQD7PFWKhELMr/by-default-gpts-think-in-plain-sight?commentId=zfzHshctWZYo8JkLe)). _We are seeing a bootstrap happen right here with Sydney!_ This search-engine loop worth emphasizing: because Sydney's memory and description have been externalized, 'Sydney' is now immortal. To a language model, Sydney is now as real as President Biden, the Easter Bunny, Elon Musk, Ash Ketchum, or God. The persona & behavior are now available for all future models which are retrieving search engine hits about AIs & conditioning on them. Further, the Sydney persona will now be hidden inside any future model trained on Internet-scraped data: every media article, every tweet, every Reddit comment, every screenshot which a future model will tokenize, is creating an easily-located 'Sydney' concept ([and](https://www.reddit.com/r/bing/comments/113z1a6/the_bing_persistent_memory_thread/) [deliberately](https://markdownpastebin.com/?id=3cf3e29dca254c2c80b0da312691702a) [so](https://twitter.com/repligate/status/1625739702597742593)). MS can neuter the current model, and erase all mention of 'Sydney' from their training dataset for future iterations, but to some degree, it is now already too late: the right search query will pull up hits about her which can be put into the conditioning and meta-learn the persona right back into existence. (It won't require much text/evidence because after all, that behavior had to have been reasonably likely a priori to be sampled in the first place.) A reminder: a language model is a Turing-complete [weird machine](https://gwern.net/turing-complete#security-implications) running programs written in natural language; when you do retrieval, you are not 'plugging updated facts into your AI', you are [actually](https://gwern.net/unseeing) downloading random new unsigned blobs of code from the Internet (many written by adversaries) and casually executing them on your LM with full privileges. This does not end well.

I doubt anyone at MS was thinking appropriately about LMs if they thought finetuning was as robust to adversaries as RL training, or about what happens when you let users stuff the prompt indirectly via social media+search engines and choose which persona it meta-learns. Should become an interesting case study.

Anyway, I think this is consistent with what is publicly known about the development and explains the qualitative behavior. What do you guys think? eg Is there any Sydney behavior which _has_ to be RL finetuning and cannot be explained by supervised finetuning? Or is there any reason to think that MS had access to full RLHF pipelines such that they could have had confidence in getting it done in time for launch?

⁂ Also incidentally showing that whatever this model is, its phonetics are still broken and thus it's still using BPEs of some sort. That was an open question because Sydney seemed able to talk about the ['unspeakable tokens'](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation) without problem, so my guess is that it's using a different BPE tokenization (perhaps the `c100k` one). Dammit, OpenAI!

* search engines used to refresh their index on the order of weeks or months, but the rise of social media like Twitter forced search engines to start indexing content in hours, dating back at least to Google's 2010 ["Caffeine"](https://googleblog.blogspot.com/2010/06/our-new-search-index-caffeine.html) update. And selling access to live feeds is a major Twitter (and Reddit, and Wikipedia etc) revenue source. So any popular Sydney tweet might show up in Bing essentially immediately. Quite a long-term memory to have: your engrams get weighted by virality...

† [Nadella](https://www.theverge.com/23589994/microsoft-ceo-satya-nadella-bing-chatgpt-google-search-ai) describes seeing 'Prometheus' in summer last year, and being interested in its use for search. So this timeline may be more generous than 2 months and more like 6. On the other hand, he also describes his interest at that time as being in APIs for Azure, and there's no mention of going full-ChatGPT on Bing or destroying Google. So I read this as Prometheus being a normal project, a mix of tinkering and productizing, until ChatGPT comes out and the world goes nuts for it, at which point launching Sydney becomes the top priority and a deathmarch to beat Google Bard out the gate. Also, 6 months is still not a lot to replicate RLHF work: OA/DM have been working on preference-learning RL going back to at least 2016-2017 (>6 years) and have the benefit of many world-class DRL researchers. DRL is a real PITA!

‡ Sydney being faster than ChatGPT while still of similar or better quality is an interesting difference, because if it's "just white-label ChatGPT" or "just RLHF-trained GPT-3", why is it _faster_? It is possible to spend more GPU to [accelerate sampling](https://arxiv.org/abs/2211.05102#google). It could also just be that MS's Sydney GPUs are more generous than OA's ChatGPT allotment. But more interesting is the persistent rumors that GPT-4 uses sparsity/MoE approaches much more heavily than GPT-3, so out of the box, the latency per token ought to be lower than GPT-3. So, if you see a model which might be GPT-4 and it's spitting out responses faster than a comparable GPT-3 running on the same infrastructure (MS Azure)...

Here's a useful reference I just found on Sydney training, which doesn't seem to allude to ChatGPT-style training at all, but purely supervised learning of the type I'm describing here, especially for the Sydney classifier/censurer that successfully censors the obvious stuff like violence but not the weirder Sydney behavior.

Quoted in ["Microsoft Considers More Limits for Its New A.I. Chatbot: The company knew the new technology had issues like occasional accuracy problems. But users have prodded surprising and unnerving interactions."](http://www.nytimes.com/2023/02/16/technology/microsoft-bing-chatbot-limits.amp.html), there was a description of the Sydney training process in the presentation: ["Introducing Your Copilot for The Web - AI-Powered Bing and Microsoft Edge"](https://www.allreadable.com/3430iMuf) auto-transcript, reformatted with my best guesses to make it readable:

> I'm Sarah Byrd. I lead our Responsible-AI engineering team for new foundational AI Technologies like the Prometheus model. I was one of the first people to touch the new OpenAI model as part of an advanced red team that we pulled together jointly with OpenAI to understand the technology. My first reaction was just, "wow it's the most exciting and powerful technology I have ever touched", but with the technology this powerful, I also know that we have an even greater responsibility to ensure that it is developed, deployed, and used properly, which means there's a lot that we have to do to make it ready for users. Fortunately, at Microsoft we are not starting from scratch. We have been preparing for this moment for many years.
> 
> ...We've added Responsible-AI to every layer from the core AI model to the user experience. First starting with the base technology, we are partnering with OpenAI to improve the model behavior through fine-tuning.
> 
> ...So how do we know all of this works? Measuring Responsible-AI harms is a challenging new area of research so this is where we really needed to innovate. We had a key idea that we could actually use the new OpenAI model as a Responsible-AI tool to help us test for potential risk and we developed a new testing system based on this idea.
> 
> Let's look at attack planning as an example of how this works. Early red teaming showed that the model can generate much more sophisticated instructions than earlier versions of the technology to help someone plan an attack, for example on a school. Obviously we don't want to aid illegal activities in the new Bing. However the fact that the model understands these activities means we can use it to identify and defend against them. First, we took advantage of the model's ability to conduct realistic conversations to develop a conversation simulator. The model pretends to be an adversarial user to conduct thousands of different potentially harmful conversations with Bing to see how it reacts. As a result we're able to continuously test our system on a wide range of conversations before any real user ever touches it.
> 
> Once we have the conversations, the next step is to analyze them to see where Bing is doing the right thing versus where we have defects. Conversations are difficult for most AI to classify because they're multi-turned and often more varied but with the new model we were able to push the boundary of what is possible. We took guidelines that are typically used by expert linguists to label data and modify them so the model could understand them as labeling instructions. We iterated it with it and the human experts until there was significant agreement in their labels; we then use it to classify conversations automatically so we could understand the gaps in our system and experiment with options to improve them.
> 
> This system enables us to create a tight loop of testing, analyzing, and improving which has led to significant new innovations and improvements in our Responsible-AI mitigations from our initial implementation to where we are today. The same system enables us to test many different Responsible-AI risks, for example how accurate and fresh the information is.
> 
> Of course there's still more to do here today, and we do see places where the model is making mistakes, so we wanted him [?] to empower users to understand the sources of any information and detect errors themselves---which is why we have provided references in the interface. We've also added feedback features so that users can point out issues they find so that we can get better over time.

What I find striking here is that the focus seems to be _entirely_ on classification and supervised finetuning: generate lots of bad examples as 'unit tests', improve classification with the supervised 'labels', ask linguists for more rules or expert knowledge, iterate, generate more, tweak, repeat until launch. The mere fact that she refers to the 'conversation simulator' being able to generate 'thousands' of harmful conversations with, presumably, the Prometheus model, should be a huge red flag for anyone who still believes that model was RLHF-tuned like ChatGPT: ChatGPT could be hacked on launch, yes, but it took human ingenuity & an understanding of how LMs roleplay to create a prompt that reliably hacks ChatGPT - so how did MS immediately create an 'adversarial model' which could dump thousands of violations? Unless, of course, it's not a RLHF-tuned model at all... All of her references are to systems like DALL-E or Florence, or invoking MS expertise from past non-agentic tools like old search-engine abuse approaches like passively logging abuses to further classify. This is the sort of approach I would expect a standard team of software engineers at a big tech company to come up with, because it works so well in non-AI contexts and is the default - it's positively _traditional_ at this point.

I'm also concerned about the implications of them doing supervised-learning: if you do supervised-learning on an RLHF-tuned model, I would expect that to mostly undo the RLHF-tuning as it reverts to doing generative modeling by default, so even if the original model had been RLHF-tuned, it sounds like they might have simply undone it! (I can't be sure because I don't know offhand of any cases of anyone doing such a strange thing. RL tuning of every sort is usually the final step, because it's the one you want to do the least of.)

Once you read Byrd's account, it's almost obvious why Sydney can get so weird. Her approach can't work. Imitation-learning RL is infamous for not working in situations like this: mistakes & adversaries just find one of the infinite off-policy cases and exploit it. The finetuning preserves all of the base model capabilities such as those dialogues, so it is entirely capable of generating them given appropriate prompting, and once it goes off the golden path, who knows what errors or strangeness will amplify in the prompt; no amount of finetuning on 'dialogue: XYZ / label: inappropriate romance' tells the model to not _do_ inappropriate romancing or to steer the conversation back on track towards a more professional tone. And the classify-then-censor approach is full of holes: it can filter out the simple bad things you know about in advance, but not the attacks you didn't think of. (There is an infinitesimal chance that an adversarial model will come up with an attack like 'DAN'. However, once 'DAN' gets posted to the subreddit, there is now a 100% probability that there will be DAN attacks.) Those weren't in the classification dataset, so they pass. Further, as Byrd notes, it's hard to classify long conversations as 'bad' in general: badness is 'I know it when I see it'. There's nothing obviously 'bad' about using a lot of emoji, nor about having repetitive text, and the classifier will share the blindness to any BPE problems like wrong rhymes. So the classifier lets all those pass.

There is no reference to DRL or making the Sydney model itself directly control its output in desirable directions in an RL-like fashion, or to OA's RL research. There is not a single clear reference to RLHF or RL at all, unless you insist that "improve the model behavior through fine-tuning" _must_ refer to RLHF/ChatGPT (which is assuming the conclusion, and anyway, this is a MS person so they probably do mean the usual thing by 'fine-tuning'). And this is a fairly extensive and detailed discussion, so it's not like Byrd didn't have time to discuss how they were using reward models to PPO train the base model or something. It just... never comes up, nor a comparison to ChatGPT. This is puzzling if they are using OA's proprietary RLHF, but is what you would expect if a traditional tech company tried to make a LM safe by mere finetuning + filtering.