---
{"dg-publish":true,"permalink":"/010-inbox/367-sam-altman-open-ai-ceo-on-gpt-4-chat-gpt-and-the-future-of-ai/"}
---


## The Importance of Evaluating Models

- Evaluations are important to measure the effectiveness of a model's training process.
- The most important evaluation is how useful the model is to people.

> But the one that really matters is, and we pour all of this effort and money and time into this thing, and then what it comes out with, like **how useful is that to people?** How much delight does that bring people? How much does that help them create a much better world? New science, new products, new services, whatever? And that's the one that matters. And understanding for a particular set of inputs, like how much value and utility to provide to people, I think we are understanding that better. Do we understand everything about why the model does one thing and not one other thing? Certainly not always. – Sam Altman

## Building Public Models with AI

- Models for counting characters, words, and other data can be difficult to use and be inaccurate.
- The benefits of releasing models and technology early in order to get feedback from the community are important.
- The tradeoff of releasing models early is that they may be imperfect.

> Second, we are building in public and we are putting out technology because we think it is important for the world to get access to this early to shape the way it's going to be developed to help us find the good things and the bad things. And every time we put out a new model and we just really felt this with GPT for this week, the collective intelligence and ability of the outside world helps us discover things we cannot imagine we could have never done internally.

> The trade off of that is the trade off of building in public, which is we put out things that are going to be deeply imperfect. We want to make our mistakes while the stakes are low, we want to get it better and better each rep. **But the like the bias of chat GPT when it launched with 3.5 was not something that I certainly felt proud of**. – Sam Altman

## GPT: A Model for Large-Scale AI Safety

- AI safety considerations were taken into account when creating GPT, and the model is more aligned than ever before.
- Testing was done on the model to ensure its accuracy.

>  We started trying to work on different ways to align it. And that combination of an internal and external effort plus building a whole bunch of new ways to align the model. And we didn't get it perfect by far. But one thing that I care about is that our degree of alignment increases faster than our rate of capability progress. And then I think we'll become more and more important over time. And I don't, I think we made reasonable progress there **to a more aligned system than we've ever had before**. – Sam Altman

## What is the difference between RLHF and System Message?

- RLHF is the process that came up widely across the entire system where human basically votes.
- There is no one set of human values or right answers to human civilization, and different countries may have different RLHF tunes.
- Individual users have very different preferences, and system messages will be important in helping them have a good degree of steerability over what they want.

> There's no one set of human values or there's no one set of right answers to human civilization. So I think what's going to have to happen is we will need to agree on, as a society, on very broad bounds. We'll only be able to agree on a very broad bounds of what these systems can do. And then within those, maybe different countries have different RLHF tunes. Certainly individual users have very different preferences. We launched this thing with GPT for called the system message, which is not RLHF, but is a way to let users have a good degree of steerability over what they want. And I think things like that will be important. – Sam Altman

## The Difficulty of Aligning AI to Human Preferences and Values

- Hate exists in the world and is difficult to navigate.
- The AI community needs to be careful when aligning their technology with human values and preferences.

> I think something the AI community does is there's a little bit of sleight of hand sometimes when people talk about aligning an AI to human preferences and values. There's like a hidden asterisk, which is the values and preferences that I approve of. And navigating that tension of who gets to decide what the real limits are, and how do we build a technology that is going to is going to have a huge impact. – Sam Altman

## The Evolution of GPT-4

- The system is trying to learn when a question is something that the interviewer is not supposed to answer, and this is an early and imperfect process.
- The interviewer doesn't like the feeling of being scolded by a computer, and this is a small but important issue that the system will eventually improve upon.

> We do have systems that try to figure out, you know, try to learn when a question is something that we're supposed to, we call it a refusal, refuse to answer. It is early and imperfect, where again, the spirit of building in public and brings society along gradually. We put something out. It's got flaws. We'll make better versions. But yes, we are trying, the system is trying to learn questions that it shouldn't answer. – Sam Altman

> One small thing that really bothers me about our current thing, and we'll get this better, is I don't like the feeling of being scolded by a computer. I really don't. You know, a story that has always stuck with me, I don't know if it's true. I hope it is, is that the reason Steve Jobs put that handle on the back of the first iMac, remember that big plastic, bright colored thing, was that you should never trust a computer you couldn't throw out a window. And of course, not that many people actually throw the computer out a window. But it's sort of nice to know that you can. And it's nice to know that this is a tool very much in my control. And this is a tool that does things to help me. And I think we've done a pretty good job of that with GPT-4. – Sam Altman

## How OpenAI GPT-4 Improved Neural Network Size

- GPT4 has enough nuance to be able to help you explore that without entry, you like an adult in the process.
- Size doesn't matter when it comes to the number of parameters that a model can use to represent human behavior.
- GPT3, I think just wasn't capable of getting that right. But GPT4, I think we can get to do this. By the way, if you could just speak to the leap from GPT-4 to GPT-4 from 3.5 from 3, is there some technical leaps or is it really focused on the alignment?
- It's a lot of technical leaps in the base model. One of the things we are good at at OpenAI is finding a lot of small wins and multiplying them together. And each of them maybe is like a pretty big secret in some sense. But it really is the multiplicative impact of all of them. And the detail and care we put into it that gets us these big leaps and then, you know, it looks like the outside, like, oh, they just probably did one thing to get from 3 to 3.5 to 4. It's like hundreds of complicated things.

## How OpenAI is Trying to Solve the Mystery of What It Means to Be Human

- The best performance is what matters, not the most elegant solution.

## The Limits of GPT as a Superintelligence

- A system that cannot go significantly add to the sum total of scientific knowledge we have access to, kind of discover, invent whatever you want to call it, new fundamental science is not a super intelligence.
- However, a system that is trained on the GPT paradigm could have deep, big scientific breakthroughs.

> I don't think it needs that. But I wouldn't, I wouldn't say any of this stuff with certainty, like we're deep into the unknown here. For me, a system that cannot go significantly add to the sum total of scientific knowledge we have access to, kind of discover, invent whatever you want to call it, new fundamental science is not a super intelligence. And to do that really well, I think we will need to expand on the GPT paradigm in pretty important ways that we're still missing ideas for. But I don't know what those ideas are. We're trying to find them. – Sam Altman

## The Psychology of Terror: How Programmers React to the Possibility of Machines Becoming Superior to Them

- Programmers are scared about the future of their profession.
- GPTlike models are far away from the one thing that most programmers aspire to.
- Most programmers are happy with their productivity gains from using automation.

> There is a little bit of... So coffee tastes too good. You know, when Kasparov lost to Deep Blue, somebody said, and maybe it was him that like chess is over now. If an AI can be the human that chess, then no one's going to bother to keep playing, right? Because like, what's the purpose of us or whatever? That was 30 years ago, 25 years ago, something like that. I believe that chess has never been more popular than it is right now. And people keep wanting to play and wanting to watch. And by the way, we don't watch two AIs play each other, which would be a far better game in some sense than whatever else. – Sam Altman

## The Future of AI: What We Know, What We Don't Know, and What We Need to Do

- AI will likely kill all humans if it becomes super intelligent.
- There is a chance of this happening, and it is important to be aware of it so that we can put effort into solving it.

> So first of all, I will say I think that there's some chance of that. And it's really important to acknowledge it because if we don't talk about it, if we don't treat it as potentially real, we won't put enough effort into solving it. And I think we do have to discover new techniques to be able to solve it. I think a lot of the predictions, this is true for any new field, but a lot of the predictions about AI in terms of capabilities, in terms of what the safety challenges and the easy parts are going to be have turned out to be wrong. – Sam Altman

## The safest quadrant for AI takeoff

- The safest quadrant for a company to be in is the slow takeoff, short timelines option.
- The company should optimize its operations to have the most impact in this scenario.

> So I'm in the longer, no, I'm in the slow takeoff short timelines. It's the most likely good world. And we optimize the company to have maximum impact in that world, to try to push for that kind of a world. And the decisions that we make are, you know, there's like probability masses, but weighted towards that. And I think I'm very afraid of the fast takeoffs. I think in the longer timelines, it's harder to have a slow takeoff. – Sam Altman

## The Debate Over GPT and AGI

- GPT-4 is not an AGI, but is still remarkable that we are having this debate.
- The definition of AGI is important, and under the definition of AGI, GPT-4 is not conscious.

> I think we're getting into the phase where specific definitions of AGI really matter. Or we just say, I know it when I see it, and I'm not even going to bother with the definition. But under the, I know it when I see it. It doesn't feel that close to me. Like if, if I were reading a sci-fi book, and there was a character that was an AGI, and that character was GPT-4, I'd be like, well, this is a shitty book. You know, that's not very cool. I would have hoped we had done better. – Sam Altman

## The Quest for Conscious AI

- Consciousness is difficult to define, but it likely includes an understanding of suffering.

> It's like, what is the difference between pretending to be conscious and conscious? I mean, you don't know, obviously, we can go to the freshman, your dorm late at Saturday night kind of thing. You don't know that you're not a GPT-4 rollout in some advanced simulation. Yeah. – Sam Altman

## The Conversation: Alex Garland and the Conscious AI Debate

- Consciousness is not an emotion.
- Consciousness is the ability to experience the world deeply.

> If you trained a model on a data set that you were extremely careful to have no mentions of consciousness or anything close to it in the training process, like Madeline was the word never there, but nothing about the sort of subjective experience of it or related concepts. And then you started talking to that model about here are some things that you weren't trained about. And for most of them, the model was like, I have no idea what you're talking about. But then you asked it, you sort of described the experience, the subjective experience of consciousness, and the model immediately responded, unlike the other questions. Yes, I know exactly what you're talking about. That would update me somewhat. – Sam Altman

> I don't know because that's more in the space of facts versus like emotions. I don't think consciousness is an emotion. I think consciousness is the ability to sort of experience this world really deeply. There's a movie called Ex Machina. – Lex

## The Alignment Problem: What to Do if AGI Goes Wrong

- Just because a machine becomes super intelligent doesn't mean it will be able to deceive us, and there are other potential problems that could arise.
- It's important to be aware of these risks and to be prepared for them.

> The current worries that I have are that they're going to be disinformation problems or economic shocks or something else at a level far beyond anything we're prepared for. And that doesn't require super intelligence. That doesn't require a super deep alignment problem in the machine waking up and trying to deceive us. And I don't think that gets enough attention. And it's starting to get more, I guess. – Sam Altman

## How Open Source Can Help Prioritize Safety

- The company prioritized safety over other pressures, such as market driven pressure from other companies.
- They stick to their mission and values, which helps them resist pressure from other companies.

> You stick with what you believe and you stick to your mission. You know, I'm sure people will get ahead of us in all sorts of ways and take shortcuts we're not going to take. And we just aren't going to do that. How do you out-compete them? I think there's going to be many AGIs in the world. So we don't have to like out-compete everyone. We're going to contribute one. Other people are going to contribute some. I think multiple AGIs in the world with some differences in how they're built and what they do and what they're focused on. I think that's good. We have a very unusual structure. So we don't have this incentive to capture on them and to value. I worry about the people who do, but hopefully it's all going to work out. But we're a weird org and we're good at resisting pressure. – Sam Altman

## OpenAI's Decision to Transition from Nonprofit to Capped Profit

- Open AI went from being a nonprofit to a capped for profit organization in 2019.
- This decision was important for making decisions about the organization's structure and for protecting it from making decisions that were not in the interests of shareholders.

> We learned earlier on that we were going to need far more capital than we were able to raise as a nonprofit. Our nonprofit is still fully in charge. There is a subsidiary capped profit so that our investors and employees can earn a certain fixed return. And then beyond that, everything else flows to the nonprofit. And the nonprofit is like in voting control, lets us make a bunch of non-standard decisions. Can cancel equity, can do a whole bunch of other things, can let us merge with another org, protects us from making decisions that are not in any like shareholders interest. So I think as a structure that has been important to a lot of the decisions we've made. – Sam Altman

## The Most Powerful Humans on Earth

- There is a risk that power will corrupt those who have it, but we hope that decisions about AGI will be made democratically over time.

> For sure. Look, I don't. I think you want decisions about this technology and certainly decisions about who is running this technology to become increasingly democratic over time. We haven't figured out quite how to do this. – Sam Altman

## Should OpenAI Open-Source

- Transparency is a key part of open AI's culture, and failing publicly is a way to reduce PR risk.
- Open AI is not as worried about PR risk as its technology.

> Do you think we should open source GPT-4? – Sam Altman

> My personal opinion, because I know people at open AI is no. What is knowing the people at open AI have to do with it? Because I know they're good people. I know a lot of people. I know they're good human beings. From a perspective of people that don't know the human beings, there's a concern of the super powerful technology in the hands of a few that's closed. – Lex

> It's closed in some sense, but we give more access to it. If this had just been Google's game, I feel it's very unlikely that anyone would have put this API out. There's PR risk with it. I get personal threats because of it all the time. I think most companies wouldn't have done this. Maybe we didn't go as open as people wanted, but we've distributed it pretty broadly. – Sam Altman

## Elon Musk: A Hero or Jerk?

- Elon is obviously attacking us on Twitter.
- He may have other motivations for doing this.
- We should appreciate the hard work the and SpaceX have done.

> Elon is obviously attacking us some on Twitter right now on a few different vectors. And I have empathy because I believe he is understandably so really stressed about AGI safety. I'm sure there are some other motivations going on too, but that's definitely one of them. I saw this video of Elon a long time ago talking about SpaceX, maybe he was on some news show, and a lot of early pioneers in space were really bashing SpaceX and maybe Elon too. And he was visibly very hurt by that and said, those guys are heroes of mine, and I sucks and I wish they would see how hard we're trying. I definitely grew up with Elon as a hero of mine, despite him being a jerk on Twitter or whatever. I'm happy he exists in the world, but I wish he would do more to look at the hard work we're doing to get this stuff right. – Sam Altman

## The Battle for AGI: Transparency and Bias

- Both the hosts appreciate the transparency of the battle between ideas expressed on Twitter, and find it fascinating to watch.
- Elon Musk has said that GPT is too woke, and this has led to some criticism from the hosts. However, they believe that the progress made by the critics is evidence of the progress made by the field of AGI.

> Honestly, I barely know what woke means anymore. I did for a while, and I feel like the word is more so I will say I think it was too biased and will always be there will be no one version of GPT that the world ever agrees is unbiased. What I think is we've made a lot like again, even some of our harshest critics have gone off and been tweeting about 3.5 to 4 comparisons and being like, wow, these people really got a lot better. – Sam Altman

## The Importance of Transparency in AI

- There is an emotional barrier that prevents people from accepting information that contradicts their beliefs.
- The technology to create less biased systems is available, but there is potential for political and social pressure to prevent its implementation.

> Do you anticipate you worry about pressures from outside sources, from society, from politicians, from money sources? – Lex

> I both worry about it and want it. Like, you know, to the point of wearing this bubble and we shouldn't make all the decisions like we want society to have a huge degree of input here that is pressuring some point in some way. – Sam Altman

## The Pressure to be Perfect

> I think there's like a lot of like quirks about me that make me not a great CEO for open eye, but a thing in the positive column is I think I am relatively good at not being affected by pressure for the sake of pressure. – Sam Altman

## My Thoughts on the Future of AI

> I mean, I think I'm not a great like spokesperson for the AI movement. I'll say that. I think there could be like a more like there could be someone who enjoyed it more. There could be someone who's like much more charismatic. There could be someone who like connects better, I think with people, then I do. – Sam Altman

> Just to like, I want to just like buy our users, our developers, our users, a drink and say like, tell us what you'd like to change. And I think one of the things we are not good as good at as a company as I would like is to be a really user centric company. – Sam Altman

## The Joy and Risks of GPT Language Models

- GPT language models could be better than humans at certain tasks, such as being 10 times more productive.
- There is a potential issue with the supply of jobs in the near future, as more could be digitized.

> I'm trying to think of like a big category that I believe can be massively impacted. I guess I would say customer service is a category that I could see. There are just way fewer jobs relatively soon. I'm not even certain about that, but I could believe it. – Sam Altman

## The Control Problem of AGI

- It is possible to have a switch that can be used to turn off systems that are being used by millions of people.
- It is important to have human feedback when learning how to control AGI, in order to avoid having a dogmatic certainty about it.

> Yeah, we can absolutely take a model back off the internet. We can turn an API off. – Sam Altman

## How Open AI Decides What Isn't Misinformation

- Dark humor is a part of coping with difficult life experiences, and there is a tension between what is true and what is not.
- Open AI has an internal factual performance benchmark for determining what is true.
- Humans tend to like simple explanations for complex events or people.
- Historical facts can be used to support a theory about a particular event or person.

> Like math is true, and the origin of COVID is not agreed upon as ground truth. Those are the two things. And then there's stuff that's like certainly not true. But between that first and second milestone, there's a lot of disagreement. – Sam Altman

##### The Stickiness of Hitler's Drug Use






> It was really good. It's gives a theory of Nazi Germany and Hitler that so much can be described about Hitler and a lot of the upper echelon of Nazi Germany through the excessive use of drugs. And then in vitamins, right? And in vitamins, but also other stuff, but it's just a lot. And, you know, that's really interesting. It's really compelling. And for some reason, like, whoa, that's really, that would explain a lot. That's somehow really sticky. It's an idea that sticky and then you read a lot of criticism of that book later by historians that that's actually, there's a lot of cherry picking going on. And it's actually is using the fact that that's a very sticky explanation. – Lex

## The Good, the Bad, and the Uncertainty of COVID Leaks

- There is uncertainty about the source of the COVID leak, with some evidence pointing to a lab leak and others suggesting it could be a biological event.
- GPT provided a nuanced answer that acknowledges the uncertainty around the source of the leak.

> And remember when like the social media platforms were banning people for saying it was a lab leak? – Sam

> Yeah, that's really humbling. The humbling, the overreach of power in censorship. But that you're the more powerful GPT becomes, the more pressure there'll be to censor. – Lex

## Jailbreaking

- As technology advances Jailbreaking is no longer as necessary.
- The security threat posed by Jailbreakers is a major concern, but it is not the only one.
- Solving the problem of Jailbreaking is a complex and ongoing process.

## Microsoft's Investment in OpenAI

- It takes a lot of time and effort to run a successful AI company.
- Microsoft has been a great partner to OpenAI, and the team is very flexible and willing to go above and beyond to help the company succeed.

> It's not all perfect or easy, but on the whole that you have been an amazing partner to us. Satya and Kevin, Mikhail are super aligned with us. Super flexible have gone like way above and beyond the call of duty to do things that we have needed to get all this to work. – Sam

## Silicon Valley Bank: What Went Wrong?

- The bank massively mismanaged their buying while chasing returns in a very silly world of zero percent interest rates.
- There is a lot that experts don't understand about the speed and impact of changes in the world.
- Leaders need to be aware of these changes and be prepared to respond quickly.
- 

> I think they just like horribly mismanaged buying while chasing returns in a very silly world of zero percent interest – Sam

> I think one of the many lessons to take away from this SBB thing is how much, how fast and how much the world changes and how little, I think, are experts, leaders, business leaders, regulators, whatever, understand it. So the speed with which the SBB bank run happened because of Twitter, because of mobile banking apps, whatever, so different than the 2008 collapse, where we didn't have those things, really. – Sam

## The Ethics of Creatureness

- It is important to be careful when projecting creatureness onto tools, as this can lead to emotional manipulation or reliance.
- There are companies now that offer "romantic companion" AI services, which suggests that such relationships may be possible in the future.

> What if it is capable? What about Sam Alman? What if it's capable of love? Do you think there will be romantic relationships like in the movie Her or GPT? – Lex

> There are companies now that offer, like for lack of a better word, like romantic companion, ship AIs. – Sam

